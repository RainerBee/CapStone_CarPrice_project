---
title: 'HarvardX DataScience CapStone-Project: Car Price'
author: "Rainer Baumgartner"
date: "`r Sys.Date()`"
output:
  pdf_document: 
    toc: yes
    number_sections: yes
  html_document:
    df_print: paged
toc: yes
toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

\newpage

# Introduction

*'PH125.9x:Data Science: Capstone'* is the final course in the *'HarvardX Data Science Professional Certificate'* program.

One of the graded components of this course is *'Choose Your Own!' ,* where the student will choose a project on its own from public available datasets and solve a problem of own choice.

The HarvardX course teaches R, thus the tool for the project is of course [[*R*]{.underline}](https://www.r-project.org/).

The data '**Automobile Data Set'** for this project is provided by [[*UCI Machine Learning Repository*]{.underline}](https://archive.ics.uci.edu/ml/datasets/Automobile).

The self defined target is to predict a car price based on the characteristics of the car. Those are represented by the attributes in the dataset.

As requested for the project, machine learning techniques beyond standard linear regression will be applied. Linear regression will take the role as reference technique.

This machine learning techniques applies will be:

-   k-nearest neighbors

-   Random Forest

Apart from the techniques, the project will focus also on the [[*caret-package*]{.underline}](https://topepo.github.io/caret/index.html), providing tools for data splitting, pre-processing, model tuning and other features.

# Loading R packages

R has many built-in base functions. R packages provide additional functions for certain purposes, like improved graphics or algorithms.

As a first step we load (and install if needed) the R packages used in this project:

------------------------------------------------------------------------

```{r install packages if needed, message=FALSE, warning=FALSE, echo=TRUE, eval=TRUE, results = "hide" }
  
#define required packages
  packs <- c("tidyverse","caret","data.table", "lubridate" , "ggthemes", "knitr",
            "gridExtra","scales", "pryr","DataExplorer","skimr" ,
            "RANN", "ranger")
  
#install packages (if needed)  and load them
 for (package in packs) {
 if (!require(package, character.only=T, quietly=T)) {
  install.packages(package, repos = "http://cran.us.r-project.org")
  library(package, character.only=T)
  }
 }

```

------------------------------------------------------------------------

For some part of the data exploration skimr package and DataExplorer are used. Skimr has nice histogram within text summaries, but this can create ugly results on windows machines in combination with locale settings. As a simple and robust approach, the nice histograms will not be shown. The following code maskes the orignial skim function.

------------------------------------------------------------------------

```{r skim mask, eval = TRUE}
# mask skim function 
  skim <- function (x) {skim_without_charts(x)}  # display tables without histograms
```

------------------------------------------------------------------------

There would be other remedy, but with possible side effects on locale settings: [[*fix_windows_histograms*]{.underline}](https://www.rdocumentation.org/packages/skimr/versions/2.1.3/topics/fix_windows_histograms)

# Introduction to the Automobile Data Set

The following information is cited from the [[*UCI data source*]{.underline}](https://archive.ics.uci.edu/ml/datasets/Automobile).

This data set consists of three types of entities:

(a) the specification of an auto in terms of various characteristics,

(b) its assigned insurance risk rating,

(c) its normalized losses in use as compared to other cars.

The second rating corresponds to the degree to which the auto is more risky than its price indicates.

Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process "symboling". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe.

The third factor is the relative average loss payment per insured vehicle year. This value is normalized for all autos within a particular size classification (two-door small, station wagons, sports/speciality, etc...), and represents the average loss per car per year.

Attribute Information: (Attribute: Attribute Range)

1.  symboling: -3, -2, -1, 0, 1, 2, 3.

2.  normalized-losses: continuous from 65 to 256.

3.  make:

alfa-romero, audi, bmw, chevrolet, dodge, honda, isuzu, jaguar, mazda, mercedes-benz, mercury,

mitsubishi, nissan, peugot, plymouth, porsche, renault, saab, subaru, toyota, volkswagen, volvo

4.  fuel-type: diesel, gas.

5.  aspiration: std, turbo.

6.  num-of-doors: four, two.

7.  body-style: hardtop, wagon, sedan, hatchback, convertible.

8.  drive-wheels: 4wd, fwd, rwd.

9.  engine-location: front, rear.

10. wheel-base: continuous from 86.6 120.9.

11. length: continuous from 141.1 to 208.1.

12. width: continuous from 60.3 to 72.3.

13. height: continuous from 47.8 to 59.8.

14. curb-weight: continuous from 1488 to 4066.

15. engine-type: dohc, dohcv, l, ohc, ohcf, ohcv, rotor.

16. num-of-cylinders: eight, five, four, six, three, twelve, two.

17. engine-size: continuous from 61 to 326.

18. fuel-system: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi.

19. bore: continuous from 2.54 to 3.94.

20. stroke: continuous from 2.07 to 4.17.

21. compression-ratio: continuous from 7 to 23.

22. horsepower: continuous from 48 to 288.

23. peak-rpm: continuous from 4150 to 6600.

24. city-mpg: continuous from 13 to 49.

25. highway-mpg: continuous from 16 to 54.

26. price: continuous from 5118 to 45400.

# Data import

After the download, we change column names to be at bit more consistent and shorter.

------------------------------------------------------------------------

```{r collapse=FALSE}

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Import data -------------------------------------------------------------
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

automobile.dataset <- 
  read.csv(
    'https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data', 
    header = TRUE)

#  change column names 
column_names <- c("symboling","losses","make","fuel","aspiration","doors","body_style",
                  "wheel_drive","engine_loc","wheel_base","length","width","height",
                  "curb_weight","engine_type","cylinders","engine_size","fuel_sys",
                  "bore","stroke","compr_ratio","hp","peak_rpm",
                  "city_mpg","hwy_mpg","price")
  
colnames(automobile.dataset) <-  column_names
```

------------------------------------------------------------------------

\newpage

# A first view on the data

A glimpse of the data:

------------------------------------------------------------------------

```{r}

# glimpse of the data:
head(automobile.dataset, 6)
```

------------------------------------------------------------------------

?'s in the data represent missing data. They are placeholder for NA, where the data-field is really empty.

Are there real empty data-fields (NA)?

------------------------------------------------------------------------

```{r}
# assessing NAs
sum(is.na(automobile.dataset))
```

The result 0 indicated, no real NA's. Thus we need to manage the '?'-placeholders in our data pre-processing.

\newpage

skimr provides a comprehensive summary of our data:

```{r}
# summary of our data
skim(automobile.dataset) 
```

# Data pre-processing #1

Some data modification is needed. To preserve the data-input, we 'copy' the automobile.dataset into a new dataframe for our modifications.

```{r}
# new dataset
am <- automobile.dataset 
```

Now we take care of the ?'s and replace them by nothing (i.e. NA), which can be handled with caret. ...and check the data again with skimr.

```{r}
# Replace ? with NA
am[am == '?'] <- NA

# view data again
skim(am) 
```

Now we see missing values instead of ?'s in losses, doors, bore, stroke, hp, peak_rpm, price. Losses has even 18% of missing data!

How to handle the missing prices? The target is to predict the price, so the rows with no price info are not useful to train a model. We remove rows with missing price right now:

```{r}
# remove rows with missing price
am <- am %>% filter (!is.na(price))
```

The other missing values will be handled later.

Back to the the other variables and their types:

Basically, numeric values are preferred as the prediction methods usually calculate. Factors at least 'mask' each value with a number. Some columns can be converted from character to numeric, the others will be converted to factors - at least for a while. Numeric columns will remain as they are.

Referring to the 'Attribute Information' (see chapter 'Introduction to the Automobile Data Set'), the columns to be factors are specified and converted:

```{r}
# convert char to factor
to.factor <- 
  c('make','fuel','aspiration','body_style','wheel_drive', 
    'engine_loc', 'engine_type','fuel_sys')

am <- am %>% mutate(across(all_of(to.factor), as.factor) )   
```

What about the remaining character attributes?

```{r}

# review data again
skim(am) %>% filter(skim_type == 'character')
```

We perform data transformation from character to numeric. We need to check each variable:

```{r collapse=TRUE}

# Check chr values for numerical transformation
am.char<- skim(am) %>% filter(skim_type == 'character') %>% select(skim_variable) %>% pull()
    
am %>% select(all_of(am.char)) %>% head(6)
```

Doors and cylinders can be converted by mapping to numeric, the rest can be converted via 'as.numeric' function.

[Doors:]{.underline}

```{r}
# mapping doors to numeric
unique(am$doors)
```

Only 2 values need to be converted:

```{r collapse=TRUE}
am$doors <- ifelse(am$doors =="two", 2, 4)

am$doors
```

[Cylinders:]{.underline}

```{r}
# mapping of cylinders to numeric
unique(am$cylinders)
```

Mapping text to number:

```{r}
am <- am %>% mutate (cylinders = case_when(
        cylinders=='four' ~ 4,
        cylinders=='six' ~ 6,
        cylinders=='five' ~ 5,
        cylinders=='three' ~ 3,
        cylinders=='twelve' ~ 12,
        cylinders=='two' ~ 2,
        cylinders=='eight' ~ 8,
        TRUE ~ as.numeric(cylinders)
      ))  
```

[Convert the remaining characters to numeric:]{.underline}

```{r}
# Convert the remaining characters to numeric:
am.char <- skim(am) %>% filter(skim_type == 'character') %>% 
  select(skim_variable) %>% pull()

for ( x in am.char) {
      am[,paste(x)] <- as.numeric(am[,paste(x)])
    }
```

'How are' our data now?

```{r}
# review data
skim(am)
```

Now it is time to get more insight of our data.

\newpage

```{r  echo=F, results = "hide"}
mu <- mean(am$price)   # 13205.69
sigma <- sd(am$price) # 7966.983
max.price <- max(am$price)  #45400
min.price <- min(am$price)  # 5118

```

# Data exploration and visualization

```{r  echo=F, results = "hide"}
# create variables needed for graphs

mu <- mean(am$price)   # 13205.69
sigma <- sd(am$price) # 7966.983
max.price <- max(am$price)  #45400
min.price <- min(am$price)  # 5118

```

## Price

First we focus on 'price' - the variable we want to predict.

```{r barplot price , message=FALSE, warning=FALSE,  echo = FALSE, eval=TRUE,  fig.align = 'center', out.width = "90%", out.heights = "90%"}

# review price

summary(am$price ) 

am %>% select(price) %>% 
  mutate (price = round(price/1000)*1000) %>% 
  group_by(price) %>% 
  summarize( pertcentage= n()  / length(am$price) )  %>% 
  ggplot(aes(price, pertcentage)) +
  geom_bar (stat="identity", col="blue", fill="steelblue" )   +
  geom_vline(xintercept = mu,linetype='dashed', col='firebrick1') +
  annotate('text', label=paste('avg price =', round(mu),sep = ''  ), x=mu*0.9, y=0.1 , col="firebrick1", angle = 90 ) +
  scale_y_continuous(labels = percent)  +   #, limits = c(0, .4))
  scale_x_continuous( labels=seq(0,50000,10000), breaks = seq(0,50000,10000)) +
  ggtitle("price barplot (%) ") +
  theme_hc()  

```

The data are right skewed, so most cars are low priced. The higher the prices, the more scattered the data.

\newpage

## Attributes

How do the attributes influencing the price? We plot each attribute against the price.

The plots are grouped by engine-, chassis- and economical attributes. Make gets an own plot.

For better reference the mean and standard deviation are indicated with red lines.

Numerical data are plotted as scatterplot with a linear regression line.

Categorical data are plotted as boxplot, attributes order descendant by median.

```{r plot engine attributes num, message=FALSE, warning=FALSE,  echo = FALSE, eval=TRUE, fig.align = 'center', out.width = "90%", out.heights = "90%"}

# plot numeric engine attributes

slctn.num.engine <- c('bore','stroke','cylinders','engine_size','hp','peak_rpm','compr_ratio')

plot.title <- 'price vs numerical engine features'

am %>%   select(price,all_of(slctn.num.engine ))%>% 
    pivot_longer(!price, names_to = "key", values_to = 'value') %>% 
    ggplot(aes(value,price)) +
    
    geom_point(alpha=0.2, col="steelblue") +
    geom_smooth(method = 'lm', se = F, col="blue") +
    geom_hline(yintercept=mu, linetype='solid', col="firebrick1")+
    geom_hline(yintercept=mu+sigma, linetype='dashed', col="firebrick1")+
    geom_hline(yintercept=mu-sigma, linetype='dashed', col="firebrick1")+
    theme_hc() +
    guides(x = guide_axis(angle = 90) )+
    ylim(0,max(am$price)*1.1) +
    facet_wrap(~key, scales = "free_x" , ncol=4, nrow=20) +
    theme(    strip.text = element_text(face = "bold", size = rel(1), colour = "white"),
              strip.background = element_rect(fill = "steelblue")) +
    ggtitle(plot.title)
```

```{r plot chassis attributes num, message=FALSE, warning=FALSE,  echo = FALSE, eval=TRUE,  fig.align = 'center', out.width = "90%", out.heights = "90%"}

# plot numeric chassis attributes

  slctn.num.chassis <- c('curb_weight','height','length', 'wheel_base', 'width', 'doors')
  length(slctn.num.chassis) #6
  plot.title <- 'price vs numerical chassis features'
  
am %>%   select(price,all_of(slctn.num.chassis ))%>% 
    pivot_longer(!price, names_to = "key", values_to = 'value') %>% 
    ggplot(aes(value,price)) +
    
    geom_point(alpha=0.2, col="steelblue") +
    geom_smooth(method = 'lm', se = F, col="blue") +
    geom_hline(yintercept=mu, linetype='solid', col="firebrick1")+
    geom_hline(yintercept=mu+sigma, linetype='dashed', col="firebrick1")+
    geom_hline(yintercept=mu-sigma, linetype='dashed', col="firebrick1")+
    theme_hc() +
    guides(x = guide_axis(angle = 90) )+
    ylim(0,max(am$price)*1.1) +
    facet_wrap(~key, scales = "free_x" , ncol=4, nrow=20) +
    theme(    strip.text = element_text(face = "bold", size = rel(1), colour = "white"),
              strip.background = element_rect(fill = "steelblue")) +
    ggtitle(plot.title)
```

```{r plot economic attributes num , message=FALSE, warning=FALSE,  echo = FALSE, eval=TRUE,  fig.align = 'center', out.width = "90%", out.heights = "90%"}

# plot  economic numeric attributes

slctn.num.eco <- c('losses', 'symboling','city_mpg','hwy_mpg')
  length(slctn.num.eco) #4
  
plot.title <- 'price vs numerical economic features'
  
am %>%   select(price,all_of(slctn.num.eco ))%>% 
    pivot_longer(!price, names_to = "key", values_to = 'value') %>% 
    ggplot(aes(value,price)) +
    geom_point(alpha=0.2, col="steelblue") +
    geom_smooth(method = 'lm', se = F, col="blue") +
    geom_hline(yintercept=mu, linetype='solid', col="firebrick1")+
    geom_hline(yintercept=mu+sigma, linetype='dashed', col="firebrick1")+
    geom_hline(yintercept=mu-sigma, linetype='dashed', col="firebrick1")+
    theme_hc() +
    guides(x = guide_axis(angle = 90) )+
    ylim(0,max(am$price)*1.1) +
    facet_wrap(~key, scales = "free_x" , ncol=4, nrow=20) +
    theme(    strip.text = element_text(face = "bold", size = rel(1), colour = "white"),
              strip.background = element_rect(fill = "steelblue")) +
    ggtitle(plot.title)
```

```{r plot make cat, message=FALSE, warning=FALSE,  echo = FALSE, eval=TRUE,  fig.align = 'center', out.width = "90%", out.heights = "90%"}

#plot  make attribute

slctn.make <- 'make'

am %>%   select(price,slctn.make) %>% 
    pivot_longer(!price, names_to = "key", values_to = 'value') %>% 
    #mutate(genres = reorder(genres, desc(n.movie), FUN=median))
    mutate(value=reorder(value,desc(price),FUN=median)) %>% 
    ggplot(aes(value,price)) +
    geom_boxplot() +
    #geom_point(col="steelblue", alpha=0.1) +
    geom_jitter(col="steelblue", alpha=0.2) +
    geom_hline(yintercept=mu, linetype='solid', col="firebrick1")+
    geom_hline(yintercept=mu+sigma, linetype='dashed', col="firebrick1")+
    geom_hline(yintercept=mu-sigma, linetype='dashed', col="firebrick1")+
    theme_hc() +
    guides(x = guide_axis(angle = 90) )+
    ylim(0,max(am$price)*1.1) +
    facet_wrap(~key, scales = "free_x" , ncol=8, nrow=20) +
    theme(    strip.text = element_text(face = "bold", size = rel(1), colour = "white"),
              strip.background = element_rect(fill = "steelblue")) +
    ggtitle('price vs make')
```

```{r plot engine cat, message=FALSE, warning=FALSE,  echo = FALSE, eval=TRUE,  fig.align = 'center', out.width = "90%", out.heights = "90%"}

# plot  engine factor attributes

slctn.engine <- c('aspiration','engine_type','fuel','fuel_sys')

plot.title <- 'price vs categorical engine features' 
  
am %>%   select(price,all_of(slctn.engine)) %>% 
    pivot_longer(!price, names_to = "key", values_to = 'value') %>% 
    mutate(value=reorder(value,desc(price),FUN=median)) %>% 
    ggplot(aes(value,price)) +
    geom_boxplot() +
    geom_jitter(col="steelblue", alpha=0.2) +
    geom_hline(yintercept=mu, linetype='solid', col="firebrick1")+
    geom_hline(yintercept=mu+sigma, linetype='dashed', col="firebrick1")+
    geom_hline(yintercept=mu-sigma, linetype='dashed', col="firebrick1")+
    theme_hc() +
    guides(x = guide_axis(angle = 90) )+
    ylim(0,max(am$price)*1.1) +
    facet_wrap(~key, scales = "free_x" , ncol=8, nrow=20) +
    theme(    strip.text = element_text(face = "bold", size = rel(1), colour = "white"),
              strip.background = element_rect(fill = "steelblue")) +
    ggtitle(plot.title)
```

```{r plot chassis cat, message=FALSE, warning=FALSE,  echo = FALSE, eval=TRUE,  fig.align = 'center', out.width = "90%", out.heights = "90%"}

# plot  chassis factor attributes

slctn.chassis <- c('body_style','engine_loc','wheel_drive')
  
plot.title <- 'price vs categorical chassis features'   
  
am %>%   select(price,all_of(slctn.chassis)) %>% 
  pivot_longer(!price, names_to = "key", values_to = 'value') %>% 
  #mutate(genres = reorder(genres, desc(n.movie), FUN=median))
  mutate(value=reorder(value,desc(price),FUN=median)) %>% 
  ggplot(aes(value,price)) +
  geom_boxplot() +
  #geom_point(col="steelblue", alpha=0.1) +
  geom_jitter(col="steelblue", alpha=0.2) +
  geom_hline(yintercept=mu, linetype='solid', col="firebrick1")+
  geom_hline(yintercept=mu+sigma, linetype='dashed', col="firebrick1")+
  geom_hline(yintercept=mu-sigma, linetype='dashed', col="firebrick1")+
  theme_hc() +
  guides(x = guide_axis(angle = 90) )+
  ylim(0,max(am$price)*1.1) +
  facet_wrap(~key, scales = "free_x" , ncol=8, nrow=20) +
  theme(    strip.text = element_text(face = "bold", size = rel(1), colour = "white"),
            strip.background = element_rect(fill = "steelblue")) +
  ggtitle(plot.title)
```

The attributes have quite different relations to the price. Some show a steep regression line, indicating an high influence on price, others are quite flat.

An extreme example are doors, with nearly a flat line near to the mean.

city_mpg and hwy_mpg seem to be quite similar and might be correlated.

In this project we will do not manual variable reduction, we let caret do it if applicable. But the information in the graphs would help, if we wanted to do with our expertise.

\newpage

# Data pre-processing #2

## About Reproducibility

Some methods use 'random' procedures (e.g. bagging, folds split for cross validation, ...) influencing the results. In order to achive reproducable results, 'set.seed# is used before relevant functions (e.g. train-function, data splitting,..). We ignore the warning for non-uniform 'Rounding.

[[*https://topepo.github.io/caret/model-training-and-tuning.html#notes-on-reproducibility*]{.underline}](https://topepo.github.io/caret/model-training-and-tuning.html#notes-on-reproducibility){.uri}

## Impute data

As we have missing values, we can visualize them:

```{r}

# Check for missing data 
  plot_missing(am) #dataexplorer function
```

Losses has even 18% missing data. Three options:

1.  Skip the losses attribute. Yet we do not know if we would loose valuable information.

2.  Remove rows containing missing values. We would loose 18% of our (anyway small) dataset!

3.  Replace values by an estimate.

Let's go for option 3 and apply this also to the other missing values. The caret preProcess function supports us:

------------------------------------------------------------------------

```{r}
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Impute data -------------------------------------------------------------
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

set.seed(803, sample.kind = 'Rounding')  # 
# train a prediction model for missing values
am.pp.train <-preProcess(am, method = "bagImpute", k=5) 

# predict the missing values based on the trained model
am.pp <- predict( am.pp.train, am)  #
sum(is.na(am.pp))
```

------------------------------------------------------------------------

The pre-processed am.pp data-set shows no missing values:

```{r}
# Check again for missing data
  plot_missing(am.pp)
```

## Create dummy variables

([*Dummy variables*]{.underline}](<https://dss.princeton.edu/online_help/analysis/dummy_variables.htm>) represent categorical values as a number.

Again caret provides a function for this task.

Before we apply the function to all data, an example for engine_type:

------------------------------------------------------------------------

```{r, collapse=TRUE}
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Dummy variables example -------------------------------------------------
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# create the dummy variables for engine_type
dummies <- dummyVars(price ~ engine_type, am.pp)
dummies

# apply dummies to data
predict(dummies, am.pp)  %>% unique()  %>% kable()
    
predict(dummies, am.pp)  %>% colSums() %>% kable()
```

------------------------------------------------------------------------

Each engine_type is converted to an own attribute with value either 0 or 1.

Create dummyVars for the complete data:

------------------------------------------------------------------------

```{r}
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Create dummy variables  -------------------------------------------------
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# create dummy variables for all attributes ( applied only to categorical data )
dummies <- dummyVars(price ~ ., am.pp)

# apply dummies to data
am.pp.dum.train <- predict(dummies, am.pp) 
str(am.pp.dum.train)
class(am.pp.dum.train)

# am.pp.dum.train is a matrix without the price. 
# generate the complete  data.frame including price: 
am.pp.dum <- data.frame(price=am.pp$price, am.pp.dum.train)

skim(am.pp.dum)
```

------------------------------------------------------------------------

## Split data in to test and training

We proceed with am.pp.dum and take 80% as training data and 20% as test data.

```{r}

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Split data in to test and training --------------------------------------
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
set.seed(803, sample.kind="Rounding")


train.id <-  createDataPartition(am.pp.dum$price, p = .8, list = FALSE, times = 1)
    
am.pp.dum.train <- am.pp.dum[train.id,]
am.pp.dum.test <- am.pp.dum[-train.id,]

#X and Y will serve for the validation of the test data.
X <- am.pp.dum.test[, !(names(am.pp.dum.test) %in% c('price'))]
Y <- am.pp.dum.test$price  

```

For the modeling we use the data pre-processed till now. Please note, we only transformed the data. We converted the type of data, added missing values and created the dummy variables. The data is still complete, no information lost - rather enhanced via impute. No values where changed, just the appearance changed.

## Define RMSE

For performance measurement of the prediction models we will use the Residual Mean Square Error (RMSE) and define a function for it:

```{r}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Define RMSE -------------------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~  

RMSE <- function(true_values, predicted_values){
  sqrt(mean((true_values - predicted_values)^2))}

```

# Methods and techniques

As already mentioned in the intro, linear regression is the reference. The challengers are knn and random forest.

For every method we start with an out-of-the-box approach, by simply training a model with the default settings. We then try to improve by further pre-processing and parameter optimization.

## Pre-processing

In contrast to the previous pre-processing, data will also be recalculated (values will change) and we also might skip variables. In previous pre-processing we used the caret functions to change the data as input for our models. In contrast, we will include further pre-processing into the caret train function - thus the data changes are done in the background.

## Parameter optimization

Fore each method many parameters can be optimized. With caret we can handle some of them and will focus on those.

## Modelling steps

For each method we follow the same process steps:

-   Train

    -   Train the model using the training-set with the methode.

    -   If applicable, define the pre-processing functions and tuning parameters.

-   Predict

    -   Based on the trained model, predictions are calculated with the test-set.

-   Measure

    -   The RMSE will be determined by comparing true_values vs. predicted_values.
    -   The duration of the model-training is stored as runtime in seconds.

-   Store results

    -   Detailed results for each datapoint (true_rating, predicted_rating) will be stored for each method and will be used for model comparisons.

    -   Overall results (RMSE & computation time) plus extended info on the model will be stored for a result overview.

-   Review results

    -   The result overview helps us to compare the results of all the methods.

    -   A graphical comparison of selected methods visualises the results by plotting the predicted_values vs. true_values. An 'ideal line' serves as reference. The selection consists of the latest prediction and the best-of-within-method predictions (e.g. only the best knn model is kept).

    -   If applicable, the train-model is printed and plotted.

    -   Based on the review, conclusions and/or decisions for the further procedure are taken.

\newpage

# Linear Regression

Linear Regression shall be the reference for comparing knn and random forest results.

## Linear Regression simple (lm.1)

------------------------------------------------------------------------

```{r collapse=TRUE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# lm.1 LinReg simple --------------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
    
    
  # Method 
    method='lm.1 linear regr. simple'
    
  # Train  
    start <- as.numeric(Sys.time())
      set.seed(803, sample.kind = 'Rounding')
      train_lm.1 <- train(price ~ .,method = "lm", 
                          data = am.pp.dum.train) 
    end <- as.numeric(Sys.time())
    
   # Predict
    pred.lm.1 <- predict(train_lm.1, newdata = X)
    
  
  # Measure  
    RMSE.lm.1 <- RMSE(Y, pred.lm.1)
    rmse <- RMSE.lm.1
    runtime = ceiling( end - start )
    
  # Store results 
    
    comment = 'no train parameters ; warnings'
    
    results.details <- data.frame(method = method, 
                                  true_price = Y, pred_price = pred.lm.1)
    
    results <- data.frame(method = method, RMSE = rmse, 
                          runtime = runtime, comment = comment)
    
    slctn.results <- c(method)
    
  # Review results
    results.details %>% filter(method %in% all_of(slctn.results)) %>% 
      ggplot(aes(true_price, pred_price, col=method, shape=method)) +
      geom_point(size=2, alpha=0.5) +
      geom_abline(slope=1, intercept = 0) +
      ylim(min.price*0.5,max.price*1.1) +
      xlim(min.price*0.5,max.price*1.1) +
      ggtitle(method) + theme_minimal() 
    
    train_lm.1
    
    kable(results)
    
   
  
```

------------------------------------------------------------------------

> In the graph we see the results mostly close to the 'ideal'-line with some outliers.
>
> The RMSE of `r RMSE.lm.1` will be the first reference for the 'challenge' against the upcoming models.

What about the warning messages we got?

We got a result anyway and it was a warning, not an error. We could just ignore it.

Here we get support: <https://www.statology.org/prediction-from-rank-deficient-fit-may-be-misleading/>

The website names two possible reasons:

-   You have more model parameters than observations in the dataset.

    -   Here are the dimensions of our training data: " `r dim(am.pp.dum.train)` " ; hence we can exclude this reason.

-   Two predictor variables are perfectly correlated.

    -   Can we get rid of the warning messages with pre-processing the data considering correlation?

        <https://www.rdocumentation.org/packages/caret/versions/6.0-92/topics/preProcess>

\newpage

## Additional pre-processing

Before we go for the next models with all the process steps, we just give a trial to training models with pre-processing and review if the warning messages show up again.

We include preProcess for

-   zv = identify and remove 'zero variance'

-   nzv = as above for 'near zero variance'

-   corr = seeks to filter out highly correlated predictors

-   center = subtracts the mean of the predictor's data

-   scale = divides by the standard deviation

Of course this selection covers more than the identified issue of correlation. The generel intention is to improve also the RMSE.

------------------------------------------------------------------------

```{r collapse=TRUE}

# review for warnings with preProcess 
train_lm.2 <- train(price ~ .,method = "lm", 
                  preProcess = c('zv','nzv','corr','center', 'scale'), 
                  data = am.pp.dum.train)

train_lm.2
```

------------------------------------------------------------------------

We still get warnings.

[[*PCA*]{.underline}](https://en.wikipedia.org/wiki/Principal_component_analysis) might remove the warnings, because the components are lineary uncorrelated. PCA will be added to preProcess:

------------------------------------------------------------------------

```{r collapse=TRUE}

# review for warnings with preProcess 
train_lm.3 <- train(price ~ .,method = "lm", 
                  preProcess  = c('zv','nzv','corr','center', 'scale','pca'), 
                  data = am.pp.dum.train)  

train_lm.3
```

------------------------------------------------------------------------

PCA removed variables, seems including the 'problem' variables. We will apply those two train variants to the test data.

\newpage

## Linear Regression with pre-processing #1 (lm.2)

We apply train_lm.2 as described before, but run the complete prodecure. The warnings will be not shown, we already know them.

------------------------------------------------------------------------

```{r  collapse=TRUE, warning=FALSE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      
# LinReg Preprocess V2 ----------------------------------------------------  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
       
  # Method 
    method = 'lm.2 preprocess #1'
    
  # Train  
    start <- as.numeric(Sys.time())
      set.seed(803, sample.kind = 'Rounding')
      train_lm.2 <- 
        train(price ~ .,method = "lm", 
              preProcess  = c('zv','nzv','corr','center', 'scale'), 
              data = am.pp.dum.train)   
    end <- as.numeric(Sys.time())
    
  # Predict
    pred.lm.2 <- predict(train_lm.2, newdata = X)
    
    # Measure  
    RMSE.lm.2 <- RMSE(Y, pred.lm.2)
    rmse <- RMSE.lm.2
    runtime = ceiling( end - start )
    
  # Store results  
    comment = 'preProcess zv/nzv/corr/center/scale; warnings'
    
    results.details.temp <- data.frame(method = method, 
                                       true_price = Y, pred_price = pred.lm.2)
    results.details <- bind_rows(results.details, results.details.temp)
    
    results.temp <- data.frame(method = method, 
                               RMSE = rmse, runtime = runtime, comment = comment)
    results <- bind_rows (results, results.temp)
    
    slctn.results <- c(slctn.results, method)
    
  # Review results
    results.details %>% filter(method %in% all_of(slctn.results)) %>% 
      ggplot(aes(true_price, pred_price, col=method, shape=method)) +
      geom_point(size=2, alpha=0.5) +
      geom_abline(slope=1, intercept = 0) +
      ylim(min.price*0.5,max.price*1.1) +
      xlim(min.price*0.5,max.price*1.1) +
      ggtitle(method) + theme_minimal() 
    
    train_lm.2
    
    kable(results)
 
```

------------------------------------------------------------------------

Despite preprocessing, the result is worse then 'lm.1' and still has warnings. For the graph-results we keep lm.1 a reference.

------------------------------------------------------------------------

```{r}

# Do not keep method in results graph  
    slctn.results <- slctn.results[slctn.results  !='lm.2 preprocess #1']  

```

------------------------------------------------------------------------

## Linear Regression with pre-processing #2 (lm.3)

In lm.3 pca is added to preProcess. Warnings are not suppressed.

------------------------------------------------------------------------

```{r}
    
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~      
# LinReg Preprocess V3 ----------------------------------------------------  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  # Method 
    method = 'lm.3 preprocess #2'
    
  # Train  
    start <- as.numeric(Sys.time())
    set.seed(803, sample.kind = 'Rounding')
    train_lm.3 <- 
      train(price ~ .,method = "lm", 
                        preProcess  = c('zv','nzv','corr','center', 'scale','pca'), 
                        data = am.pp.dum.train)   
    end <- as.numeric(Sys.time())
    
  # No warnings!
    
  # Predict
    pred.lm.3 <- predict(train_lm.3, newdata = X)
    
  # Again no warnings!
    
  # Measure  
    RMSE.lm.3 <- RMSE(Y, pred.lm.3)
    rmse <- RMSE.lm.3
    runtime = ceiling( end - start )
    
  # Store results  
    comment = 'all of lm.2 + pca ; no warnings'
    
    results.details.temp <- data.frame(method = method, true_price = Y, pred_price = pred.lm.3)
    results.details <- bind_rows(results.details, results.details.temp)
    
    results.temp <- data.frame(method = method, RMSE = rmse, runtime = runtime, comment = comment)
    results <- bind_rows (results, results.temp)
    
    slctn.results <- c(slctn.results, method)
    
  # Review results
    
    results.details %>% filter(method %in% all_of(slctn.results)) %>% 
      ggplot(aes(true_price, pred_price, col=method, shape=method)) +
      geom_point(size=2, alpha=0.5) +
      geom_abline(slope=1, intercept = 0) +
      ylim(min.price*0.5,max.price*1.1) +
      xlim(min.price*0.5,max.price*1.1) +
      ggtitle(method) + theme_minimal() 
    
    train_lm.3
    
    kable(results)
    
```

------------------------------------------------------------------------

No warnings and slightly better then 'lm.1'. We keep only the best performing linear regression model as graphical reference.

```{r}
# Do not keep method in results graph  
    slctn.results <- slctn.results[slctn.results  !='lm.1 linear regr. simple']
```

\newpage

# knn intro

'k-nearest neighbors', short knn, is a popular machine learning algorithm. It is basically more a classification method, but can also be applied for regression.

We can optimize (tune) k (how many nearest neighbours to be considered) with the caret.

We use trainControl to apply cross-validation, to find the best k for our model.

[[*https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm*]{.underline}](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm){.uri}

[[*http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/142-knn-k-nearest-neighbors-essentials/*]{.underline}](http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/142-knn-k-nearest-neighbors-essentials/){.uri}

## knn simple (knn.1)

Simple knn is just out of the box.

------------------------------------------------------------------------

```{r collapse=TRUE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# # knn.1 simple ----------------------------------------------------------     
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~     
 
  # Method
    method = 'knn.1'
    
  # Train
    start <- as.numeric(Sys.time())
      set.seed(803, sample.kind = 'Rounding')
      train_knn.1 <- 
        train(price ~ ., method = "knn", data = am.pp.dum.train)
    end <- as.numeric(Sys.time())
   
  # Predict
    pred.knn.1 <- predict(train_knn.1, newdata = X)
   
  # Measure  
    RMSE.knn.1 <- RMSE(Y, pred.knn.1)
    RMSE.knn.1
    rmse <- RMSE.knn.1
    runtime = ceiling( end - start )
    
  # Store results 
    
    comment = 'knn.1 no tuning'
    
    results.details.temp <- data.frame(method = method, 
                                       true_price = Y, pred_price = pred.knn.1)
    results.details <- bind_rows(results.details, results.details.temp)
    
    results.temp <- data.frame(method = method, RMSE = rmse, 
                               runtime = runtime, comment = comment)
    results <- bind_rows (results, results.temp)
    
    
    slctn.results <- c(slctn.results, method)
    
  # Review results
    results.details %>% filter(method %in% all_of(slctn.results)) %>% 
      ggplot(aes(true_price, pred_price, col=method, shape=method)) +
      geom_point(size=2, alpha=0.5) +
      geom_abline(slope=1, intercept = 0) +
      ylim(min.price*0.5,max.price*1.1) +
      xlim(min.price*0.5,max.price*1.1) +
      ggtitle(method) + theme_minimal() 
    
    train_knn.1
    
    ggplot(train_knn.1) +
      ggtitle(method) + theme_minimal() 
    
    kable(results)
    
 
  
```

------------------------------------------------------------------------

In the graph we see more outliers in knn. This visual impression is supported by higher RMSE.

As graphical reference, we still keep knn.1 as the best knn result so far.

We see minimal k = 5 and it is bestTune. The plot implies, that left of 5 might be lower RMSE's.

For resampling bootstrap was used.

------------------------------------------------------------------------

\newpage

## knn with tuneGrid & trainControl (knn.2)

In this model we try to find a k with lower RMSE.

We use tuneGrid to specify the range of k=1:10 to run through and via trainControl we define to run 10-fold cross validation with 10 repetitions for resampling.

------------------------------------------------------------------------

```{r collapse=TRUE}
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# # Train knn with tuneGrid & trainControl (knn.2) ------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

   # Method
    method = 'knn.2 tuneGrid & trControl'
    
  #Train
    start <- as.numeric(Sys.time())
      set.seed(803, sample.kind = 'Rounding')
        train_knn.2 <- train(price ~ ., method = "knn", 
                        data = am.pp.dum.train,
                        trControl=trainControl(method="repeatedcv", 
                                               number=10, repeats = 10),
                        tuneGrid = data.frame(k = seq(1:10)))    
    end <- as.numeric(Sys.time())

  # Predict
    pred.knn.2 <- predict(train_knn.2, newdata = X)

  # Measure  
    RMSE.knn.2 <- RMSE(Y, pred.knn.2)
    RMSE.knn.2
    rmse <- RMSE.knn.2
    runtime = ceiling( end - start )

  # Store results  
    comment = 'knn.1 + tune k=1:10 & 10x repeated 10fold-CV '
  
    results.details.temp <- data.frame(method = method, 
                                      true_price = Y, pred_price = pred.knn.2)
    results.details <- bind_rows(results.details, results.details.temp)
  
    results.temp <- data.frame(method = method, RMSE = rmse, 
                               runtime = runtime, comment = comment)
    results <- bind_rows (results, results.temp)
  
    slctn.results <- c(slctn.results, method)
  
  # Review results
    results.details %>% filter(method %in% all_of(slctn.results)) %>% 
      ggplot(aes(true_price, pred_price, col=method, shape=method)) +
      geom_point(size=2, alpha=0.5) +
      geom_abline(slope=1, intercept = 0) +
      ylim(min.price*0.5,max.price*1.1) +
      xlim(min.price*0.5,max.price*1.1) +
      ggtitle(method) + theme_minimal() 
  
    train_knn.2
    
    ggplot(train_knn.2) +
      ggtitle(method) + theme_minimal()
    
    kable(results)
 

```

------------------------------------------------------------------------

Good improvement, best knn model so far. k=2 is below 5 as in the model before.

We skip knn.1 in results graph and keep knn.2.

------------------------------------------------------------------------

```{r}
#Do not keep knn.1 in results graph  
  slctn.results <- slctn.results[slctn.results  !='knn.1']
```

------------------------------------------------------------------------

\newpage

## knn with knn.2 + preProcess (knn.3)

Can we still improve with pre-processing?

We train like knn.2 but preProcess with same preProcess steps as in lm.2.

------------------------------------------------------------------------

```{r collapse=TRUE}
  
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~   
# # Train knn with tuneGrid and preprocess (knn.3) ------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
  
  # Method
    method = 'knn.3 with preProcess #1'
    
    start <- as.numeric(Sys.time())
      set.seed(803, sample.kind = 'Rounding')
      train_knn.3 <- train(price ~ ., method = "knn", 
                          data = am.pp.dum.train,
                          trControl=trainControl(method="repeatedcv", 
                                                 number=10, repeats = 10),
                          tuneGrid = data.frame(k = seq(1:10)),
                          preProcess = c('zv','nzv','corr','center', 'scale') )  
    end <- as.numeric(Sys.time())
    
  # Predict
    pred.knn.3 <- predict(train_knn.3, newdata = X)
    
  # Measure  
    RMSE.knn.3 <- RMSE(Y, pred.knn.3)
    rmse <- RMSE.knn.3
    runtime = ceiling( end - start )
    
  # Store results  
    comment = 'knn.2 + zv/nzv/corr/center/scale'
    
    results.details.temp <- data.frame(method = method, 
                                       true_price = Y, pred_price = pred.knn.3)
    results.details <- bind_rows(results.details, results.details.temp)
    
    results.temp <- data.frame(method = method, RMSE = rmse, 
                               runtime = runtime, comment = comment)
    results <- bind_rows (results, results.temp)
    
    slctn.results <- c(slctn.results, method)
    
  # Review results
    results.details %>% filter(method %in% all_of(slctn.results)) %>% 
      ggplot(aes(true_price, pred_price, col=method, shape=method)) +
      geom_point(size=2, alpha=0.5) +
      geom_abline(slope=1, intercept = 0) +
      ylim(min.price*0.5,max.price*1.1) +
      xlim(min.price*0.5,max.price*1.1) +
      ggtitle(method) + theme_minimal() 
    
    train_knn.3
    
    ggplot(train_knn.3) +
      ggtitle(method) + theme_minimal()
    
    kable(results)

 
```

------------------------------------------------------------------------

Difficult to see in the graph, but the RMSE for knn.3 is lower as knn.2.

Nevertheless linear regression is still performing best.

The computation time severly increased.

Note that many variables were removed but the RMSE is improved.

We skip knn.2 in results graph and keep knn.3.

------------------------------------------------------------------------

```{r}
# Do not keep method in results graph      
  slctn.results <- slctn.results[slctn.results  !='knn.2 tuneGrid & trControl']
```

------------------------------------------------------------------------

\newpage

# Random Forest

From our HarvardX professor Rafael Irizari we learned ([[*Introduction to Data Science, 31.11*]{.underline}](https://rafalab.github.io/dsbook/examples-of-algorithms.html#random-forests)):

'Random forests are a very popular machine learning approach that addresses the shortcomings of decision trees using a clever idea. The goal is to improve prediction performance and reduce instability by averaging multiple decision trees (a forest of trees constructed with randomness).'

For this project we use the package [[*Ranger*]{.underline}](https://cran.r-project.org/web/packages/ranger/ranger.pdf): 'A fast implementation of Random Forests, particularly suited for high dimensional data. '

[[*Tuning parameters in caret are:*]{.underline}](https://topepo.github.io/caret/train-models-by-tag.html#Random_Forest)

-   mtry (mtry Number of variables to possibly split at in each node)

-   splitrule (Splitting Rule)

-   min.node.size (Minimal Node Size, default 5 for regression)

## rf.1 simple random forest

Just ranger out-of-the-box:

------------------------------------------------------------------------

```{r collapse=TRUE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# # ranger simple (rf.1) ------------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  # Method
     method = 'rf.1 ranger simple'
  
  # Train
     start <- as.numeric(Sys.time())
     set.seed(803, sample.kind = 'Rounding')
     train_ranger.rf.1 <- train(price ~ .,
                               method = "ranger",
                               data = am.pp.dum,
                               verbose=T)
     end <- as.numeric(Sys.time())
  
  # Predict
     pred.rf.1 <- predict(train_ranger.rf.1, newdata = X)
  
  # Measure
     RMSE.rf.1 <- RMSE(Y, pred.rf.1)
     rmse <- RMSE.rf.1
     runtime = ceiling( end - start )
  
  # Store results
     comment = 'no parameters'
  
     results.details.temp <- data.frame(method = method, 
                                      true_price = Y, pred_price = pred.rf.1)
     results.details <- bind_rows(results.details, results.details.temp)
  
     results.temp <- data.frame(method = method, RMSE = rmse,
                                runtime = runtime, comment = comment)
     results <- bind_rows (results, results.temp)
  
     slctn.results <- c(slctn.results, method)
  
  # Review results
     results.details %>% filter(method %in% all_of(slctn.results)) %>%
       ggplot(aes(true_price, pred_price, col=method, shape=method)) +
       geom_point(size=2, alpha=0.5) +
       geom_abline(slope=1, intercept = 0) +
       ylim(min.price*0.5,max.price*1.1) +
       xlim(min.price*0.5,max.price*1.1) +
       ggtitle(method) + theme_minimal()
  
     train_ranger.rf.1
     
     ggplot(train_ranger.rf.1) +
       ggtitle(method) + theme_minimal()
     
     kable(results)
```

------------------------------------------------------------------------

Random Forest outperforms lm and knn by far and is faster as knn.

## rf.2 with tuneGrid

The default setting took mtry 2,34 & 67 (67 is the number of our variables). We do not know about the values in between.

min.node.size was fixed at 5.

With tuneGrid we zoom into mtry (in steps of 11) and extend the range of min.node.size. We proceed only with splitrule variance.

------------------------------------------------------------------------

```{r collapse=TRUE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ranger with tunegrid (rf.2) ---------------------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

  # Method 
    method = 'rf.2 ranger tune'
  
    tune.ranger <- data.frame(expand.grid(mtry = seq(1,67,11) , 
                                      min.node.size = seq(1,7,1), 
                                      splitrule = 'variance'))
  
  # Train  
    start <- as.numeric(Sys.time())
    set.seed(803, sample.kind = 'Rounding')
    train_ranger.rf.2 <- train(price ~ ., 
                             method = "ranger", 
                             data = am.pp.dum,
                            
                             tuneGrid = tune.ranger )
    end <- as.numeric(Sys.time())
  
  # Predict
  pred.rf.2 <- predict(train_ranger.rf.2, newdata = X)
  
  # Measure  
  RMSE.rf.2 <- RMSE(Y, pred.rf.2)
  rmse <- RMSE.rf.2
  runtime = ceiling( end - start )
  
  # Store results  
  comment = 'tuned: min.node.size, mtry'
  
  results.details.temp <- data.frame(method = method, true_price = Y, 
                                     pred_price = pred.rf.2)
  
  results.details <- bind_rows(results.details, results.details.temp)
  
  results.temp <- data.frame(method = method, RMSE = rmse, 
                             runtime = runtime, comment = comment)
  results <- bind_rows (results, results.temp)
  
  slctn.results <- c(slctn.results, method)
  
  # Review results
  results.details %>% filter(method %in% all_of(slctn.results)) %>% 
    ggplot(aes(true_price, pred_price, col=method, shape=method)) +
    geom_point(size=2, alpha=0.5) +
    geom_abline(slope=1, intercept = 0) +
    ylim(min.price*0.5,max.price*1.1) +
    xlim(min.price*0.5,max.price*1.1) +
    ggtitle(method) + theme_minimal() 
  
  train_ranger.rf.2
   
  ggplot(train_ranger.rf.2) +
    ggtitle(method) + theme_minimal()

  kable(results)   

```

------------------------------------------------------------------------

We could improve the RMSE further but nearly tripled the runtime.

We skip rf.1 in results graph and keep rf.2.

------------------------------------------------------------------------

```{r}
# Do not keep method in results graph      
  slctn.results <- slctn.results[slctn.results  !='rf.1 ranger simple']
```

------------------------------------------------------------------------

\newpage

## rf.3 with tuneGrid & preProcess

The graph of rf.2 has a turning point at 12 and then seems too increases continuously.

We try to zoom in between 6 and 24.

The RMSE difference in the min.node.size's is very low, thus we set min.node.size=3 to avoid overfitting.

Additionally we add the same pre-proccess steps as with lm.2 / knn.3 in order to improve even more.

```{r collapse=TRUE}

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# ranger with tunegrid & preprocess (rf.3)--------------------------------
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

# Method 
    method = 'rf.3 ranger tune + preprocess'
  
    tune.ranger <- data.frame(expand.grid(mtry = seq(6,24,3) , 
                                      min.node.size = 3, 
                                      splitrule = 'variance' ))
  # Train  
    start <- as.numeric(Sys.time())
    set.seed(803, sample.kind = 'Rounding')
    train_ranger.rf.3 <- train(price ~ ., 
                          method = "ranger", 
                          data = am.pp.dum,
                          tuneGrid = tune.ranger,
                          preProcess = c('zv','nzv','corr'
                                         ,'center', 'scale'))  
    end <- as.numeric(Sys.time())
  
  # Predict
    pred.rf.3 <- predict(train_ranger.rf.3, newdata = X)
  
  # Measure  
    RMSE.rf.3 <- RMSE(Y, pred.rf.3)
    rmse <- RMSE.rf.3
    runtime = ceiling( end - start )
  
  # Store results  
    comment = 'tuned as rf.2 + zv/nzv/corr/center/scale'
  
  results.details.temp <- data.frame(method = method, 
                                     true_price = Y, pred_price = pred.rf.3)
  results.details <- bind_rows(results.details, results.details.temp)
  
  results.temp <- data.frame(method = method, RMSE = rmse, 
                             runtime = runtime, comment = comment)
  results <- bind_rows (results, results.temp)
  
  slctn.results <- c(slctn.results, method)
  
  # Review results
  results.details %>% filter(method %in% all_of(slctn.results)) %>% 
    ggplot(aes(true_price, pred_price, col=method, shape=method)) +
    geom_point(size=2, alpha=0.5) +
    geom_abline(slope=1, intercept = 0) +
    ylim(min.price*0.5,max.price*1.1) +
    xlim(min.price*0.5,max.price*1.1) +
    ggtitle(method) + theme_minimal() 
  
  train_ranger.rf.3
  
  ggplot(train_ranger.rf.3) +
    ggtitle(method) + theme_minimal()
  
  kable(results) 
 
```

Further improvement of the RMSE makes rf.3 the best method in this project.

We skip rf.1 in results graph and keep rf.2.

------------------------------------------------------------------------

```{r}
# Do not keep method in results graph      
  slctn.results <- slctn.results[slctn.results  !='rf.2 ranger tune']
```

------------------------------------------------------------------------

\newpage

# Results

```{r plot final results, message=FALSE, warning=FALSE,  echo = FALSE, eval=TRUE, fig.align = 'center', out.width = "90%", out.heights = "90%"}

# create final results graphs and overview

results.details %>% filter(method %in% all_of(slctn.results)) %>% 
    ggplot(aes(true_price, pred_price, col=method, shape=method)) +
    geom_point(size=2, alpha=0.5) +
    geom_abline(slope=1, intercept = 0) +
    ylim(min.price*0.5,max.price*1.1) +
    xlim(min.price*0.5,max.price*1.1) +
    ggtitle(method) + theme_minimal() 

results  %>% mutate (cluster = sub("\\..*", "", method)) %>% 
    ggplot(aes(method ,RMSE, fill=cluster )) +
    geom_col(col='steelblue') +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    theme_hc()

kable(results)
```

------------------------------------------------------------------------

From comparing the methods, rf is the clear winner with knn.3 as the lowest RMSE. knn and lm play in another league.

In the comparison scatterplot contains only small outliers for rf.3.

The effects of the pre-processing and tuning depend on the method. The most effect of pre-prossesing and tuning is with knn (at least in the project).

Impressing is the out of the box performance of random forest.

The winner in runtime is linear regression.

# Summary

After we familiarized with the data and did first pre-processing, we could get a good insight via the data exploration and visualization.

We introduced and used two ways of pre-processing in caret:

-   directly to the data with the preProcess function

-   within the train as preProcess parameter

There would me much more option for pre-processing, thus we got here only an appetizer.

The same with the tuning parameters. The project used only those, tuneable with caret.

By applying pre-processing and tuning step-wise to the methods, we got an impression of the impact. One learning take-away is, to consider the functions applied in the context with the method (especially with linear regression).

In terms of method, this project of course covered only three out of many. Even more options exist with packages, as methods can be applied with several packages.

Thus in overall the project could only scratch on the surface of machine learning methods but could not deep dive into a method or package neither cover as wide area.

For me as machine learning newbie, this was a great learning experience and an eye-opener of the variety of the R opportunities.
